"use strict";(self.webpackChunkq_01_docs=self.webpackChunkq_01_docs||[]).push([[2408],{3316:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>c,toc:()=>l});var s=r(7624),t=r(2172);const a={title:"Batch Processing",sidebar_label:"Batch Processing",sidebar_position:10,description:"Large-scale data operations and bulk processing"},i="Batch Processing",c={id:"products/map.q01.io/core-api-platform-v4/advanced-topics/batch-processing",title:"Batch Processing",description:"Large-scale data operations and bulk processing",source:"@site/docs/products/map.q01.io/core-api-platform-v4/06-advanced-topics/batch-processing.md",sourceDirName:"products/map.q01.io/core-api-platform-v4/06-advanced-topics",slug:"/products/map.q01.io/core-api-platform-v4/advanced-topics/batch-processing",permalink:"/docs/products/map.q01.io/core-api-platform-v4/advanced-topics/batch-processing",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:10,frontMatter:{title:"Batch Processing",sidebar_label:"Batch Processing",sidebar_position:10,description:"Large-scale data operations and bulk processing"},sidebar:"mapSidebar",previous:{title:"Internal Architecture",permalink:"/docs/products/map.q01.io/core-api-platform-v4/advanced-topics/internal-architecture"},next:{title:"Best Practices",permalink:"/docs/products/map.q01.io/core-api-platform-v4/best-practices/"}},o={},l=[{value:"Overview",id:"overview",level:2},{value:"Batch Insert",id:"batch-insert",level:2},{value:"Single Batch Insert",id:"single-batch-insert",level:3},{value:"Batch Insert Implementation",id:"batch-insert-implementation",level:3},{value:"Backend Batch Insert (PHP)",id:"backend-batch-insert-php",level:3},{value:"Batch Update",id:"batch-update",level:2},{value:"Bulk Update by IDs",id:"bulk-update-by-ids",level:3},{value:"Bulk Update by Filter",id:"bulk-update-by-filter",level:3},{value:"Batch Update Implementation",id:"batch-update-implementation",level:3},{value:"Batch Delete",id:"batch-delete",level:2},{value:"Bulk Delete by IDs",id:"bulk-delete-by-ids",level:3},{value:"Bulk Delete by Filter",id:"bulk-delete-by-filter",level:3},{value:"Transaction Management",id:"transaction-management",level:2},{value:"All-or-Nothing Batches",id:"all-or-nothing-batches",level:3},{value:"Partial Commit Batches",id:"partial-commit-batches",level:3},{value:"Progress Tracking",id:"progress-tracking",level:2},{value:"Progress Bar Implementation",id:"progress-bar-implementation",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Retry Failed Batches",id:"retry-failed-batches",level:3},{value:"Partial Failure Recovery",id:"partial-failure-recovery",level:3},{value:"Parallel Processing",id:"parallel-processing",level:2},{value:"Concurrent Batches",id:"concurrent-batches",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Database-Level Optimizations",id:"database-level-optimizations",level:3},{value:"Batch Size Tuning",id:"batch-size-tuning",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"\u2705 DO:",id:"-do",level:3},{value:"\u274c DON&#39;T:",id:"-dont",level:3},{value:"Summary",id:"summary",level:2},{value:"Related Concepts",id:"related-concepts",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.M)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Batch processing"})," enables efficient handling of large-scale data operations, allowing you to insert, update, or delete thousands of records with optimal performance and transaction management."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Use Cases:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data migration from external systems"}),"\n",(0,s.jsx)(n.li,{children:"Bulk product catalog updates"}),"\n",(0,s.jsx)(n.li,{children:"Mass price adjustments"}),"\n",(0,s.jsx)(n.li,{children:"End-of-day order processing"}),"\n",(0,s.jsx)(n.li,{children:"Periodic data synchronization"}),"\n",(0,s.jsx)(n.li,{children:"Bulk user imports"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Higher throughput (process 1000s of records)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Lower overhead (fewer HTTP requests)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Transaction management (all-or-nothing commits)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Progress tracking (monitor batch execution)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Error handling (partial failure recovery)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"batch-insert",children:"Batch Insert"}),"\n",(0,s.jsx)(n.h3,{id:"single-batch-insert",children:"Single Batch Insert"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"API endpoint:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Batch insert multiple products\ncurl -X POST https://api.q01.io/api/v4/core/products/batch \\\n  -H "Authorization: Bearer $TOKEN" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "records": [\n      {"XPRD01": "Widget A", "XPRD02": 19.99, "XPRD05": 1},\n      {"XPRD01": "Widget B", "XPRD02": 29.99, "XPRD05": 1},\n      {"XPRD01": "Widget C", "XPRD02": 39.99, "XPRD05": 2},\n      ...\n    ]\n  }\'\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Response:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "inserted": 1000,\n  "failed": 0,\n  "errors": [],\n  "duration_ms": 1250,\n  "records": [\n    {"PRD_ID": 1001, "XPRD01": "Widget A", "XPRD02": 19.99},\n    {"PRD_ID": 1002, "XPRD01": "Widget B", "XPRD02": 29.99},\n    {"PRD_ID": 1003, "XPRD01": "Widget C", "XPRD02": 39.99},\n    ...\n  ]\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"batch-insert-implementation",children:"Batch Insert Implementation"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"JavaScript:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:'class BatchProcessor {\n  constructor(apiClient, batchSize = 100) {\n    this.apiClient = apiClient;\n    this.batchSize = batchSize;\n  }\n\n  async batchInsert(dimension, records) {\n    const batches = this.chunk(records, this.batchSize);\n    const results = {\n      total: records.length,\n      inserted: 0,\n      failed: 0,\n      errors: []\n    };\n\n    console.log(`Processing ${batches.length} batches...`);\n\n    for (let i = 0; i < batches.length; i++) {\n      const batch = batches[i];\n\n      console.log(`Batch ${i + 1}/${batches.length}: ${batch.length} records`);\n\n      try {\n        const response = await this.apiClient.post(\n          `/api/v4/core/${dimension}/batch`,\n          { records: batch }\n        );\n\n        results.inserted += response.inserted;\n        results.failed += response.failed;\n        results.errors.push(...response.errors);\n\n        console.log(`\u2705 Batch ${i + 1} completed: ${response.inserted} inserted`);\n      } catch (error) {\n        console.error(`\u274c Batch ${i + 1} failed: ${error.message}`);\n        results.failed += batch.length;\n        results.errors.push({\n          batch: i + 1,\n          error: error.message,\n          records: batch\n        });\n      }\n    }\n\n    return results;\n  }\n\n  chunk(array, size) {\n    const chunks = [];\n    for (let i = 0; i < array.length; i += size) {\n      chunks.push(array.slice(i, i + size));\n    }\n    return chunks;\n  }\n}\n\n// Usage\nconst processor = new BatchProcessor(apiClient, 100);\n\nconst products = [\n  {XPRD01: "Product 1", XPRD02: 19.99, XPRD05: 1},\n  {XPRD01: "Product 2", XPRD02: 29.99, XPRD05: 1},\n  // ... 10,000 products\n];\n\nconst results = await processor.batchInsert(\'products\', products);\n\nconsole.log(`Inserted: ${results.inserted}`);\nconsole.log(`Failed: ${results.failed}`);\nconsole.log(`Errors: ${results.errors.length}`);\n'})}),"\n",(0,s.jsx)(n.h3,{id:"backend-batch-insert-php",children:"Backend Batch Insert (PHP)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-php",children:"class BatchInsertHandler {\n    private $db;\n    private $batchSize = 100;\n\n    public function batchInsert(string $dimension, array $records): array {\n        $inserted = 0;\n        $failed = 0;\n        $errors = [];\n\n        // Process in batches\n        $batches = array_chunk($records, $this->batchSize);\n\n        foreach ($batches as $batchIndex => $batch) {\n            try {\n                // Start transaction\n                $this->db->beginTransaction();\n\n                // Build bulk insert SQL\n                $sql = $this->buildBulkInsertSQL($dimension, $batch);\n\n                // Execute\n                $this->db->exec($sql);\n\n                // Insert OUTBOX events\n                foreach ($batch as $record) {\n                    $this->insertOutboxEvent('ProductCreated', $dimension, $record);\n                }\n\n                // Commit\n                $this->db->commit();\n\n                $inserted += count($batch);\n\n            } catch (\\Exception $e) {\n                // Rollback on error\n                $this->db->rollBack();\n\n                $failed += count($batch);\n                $errors[] = [\n                    'batch' => $batchIndex + 1,\n                    'error' => $e->getMessage(),\n                    'record_count' => count($batch)\n                ];\n            }\n        }\n\n        return [\n            'inserted' => $inserted,\n            'failed' => $failed,\n            'errors' => $errors\n        ];\n    }\n\n    private function buildBulkInsertSQL(string $dimension, array $records): string {\n        $tableName = \"TB_ANAG_{$dimension}00\";\n\n        // Get field names from first record\n        $fields = array_keys($records[0]);\n        $fieldList = implode(', ', $fields);\n\n        // Build VALUES clauses\n        $values = [];\n        foreach ($records as $record) {\n            $valueList = implode(', ', array_map(function($value) {\n                return $this->db->quote($value);\n            }, array_values($record)));\n\n            $values[] = \"($valueList)\";\n        }\n\n        $valuesString = implode(', ', $values);\n\n        return \"INSERT INTO $tableName ($fieldList) VALUES $valuesString\";\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"batch-update",children:"Batch Update"}),"\n",(0,s.jsx)(n.h3,{id:"bulk-update-by-ids",children:"Bulk Update by IDs"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Update multiple products\ncurl -X PUT https://api.q01.io/api/v4/core/products/batch \\\n  -H "Authorization: Bearer $TOKEN" \\\n  -d \'{\n    "records": [\n      {"PRD_ID": 1001, "XPRD02": 22.99, "XPRD06": "Y"},\n      {"PRD_ID": 1002, "XPRD02": 32.99, "XPRD06": "Y"},\n      {"PRD_ID": 1003, "XPRD02": 42.99, "XPRD06": "N"}\n    ]\n  }\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"bulk-update-by-filter",children:"Bulk Update by Filter"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Update all products in category 5\ncurl -X PATCH https://api.q01.io/api/v4/core/products/batch \\\n  -H "Authorization: Bearer $TOKEN" \\\n  -d \'{\n    "filters": {\n      "XPRD05": 5,\n      "TREC": "N"\n    },\n    "updates": {\n      "XPRD06": "Y",\n      "XPRD10": "2025-12-19"\n    }\n  }\'\n\n# Response\n{\n  "updated": 250,\n  "duration_ms": 450\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"batch-update-implementation",children:"Batch Update Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"async function batchUpdate(dimension, updates) {\n  const batchSize = 100;\n  const batches = chunk(updates, batchSize);\n\n  let totalUpdated = 0;\n\n  for (const batch of batches) {\n    const response = await apiClient.put(\n      `/api/v4/core/${dimension}/batch`,\n      { records: batch }\n    );\n\n    totalUpdated += response.updated;\n\n    console.log(`Updated: ${response.updated} records`);\n  }\n\n  return totalUpdated;\n}\n\n// Usage\nconst updates = [\n  {PRD_ID: 1001, XPRD02: 22.99},\n  {PRD_ID: 1002, XPRD02: 32.99},\n  // ... 5,000 updates\n];\n\nawait batchUpdate('products', updates);\n"})}),"\n",(0,s.jsx)(n.h2,{id:"batch-delete",children:"Batch Delete"}),"\n",(0,s.jsx)(n.h3,{id:"bulk-delete-by-ids",children:"Bulk Delete by IDs"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Delete multiple products\ncurl -X DELETE https://api.q01.io/api/v4/core/products/batch \\\n  -H "Authorization: Bearer $TOKEN" \\\n  -d \'{\n    "ids": [1001, 1002, 1003, 1004, 1005]\n  }\'\n\n# Response\n{\n  "deleted": 5,\n  "duration_ms": 120\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"bulk-delete-by-filter",children:"Bulk Delete by Filter"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Delete all inactive products older than 2 years\ncurl -X DELETE https://api.q01.io/api/v4/core/products/batch \\\n  -H "Authorization: Bearer $TOKEN" \\\n  -d \'{\n    "filters": {\n      "XPRD06": "N",\n      "CREATED_AT": {"$lt": "2023-01-01"}\n    }\n  }\'\n\n# Response\n{\n  "deleted": 1250,\n  "duration_ms": 2500\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"transaction-management",children:"Transaction Management"}),"\n",(0,s.jsx)(n.h3,{id:"all-or-nothing-batches",children:"All-or-Nothing Batches"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"class TransactionalBatch {\n  async processBatch(dimension, records) {\n    // All records in single transaction\n    try {\n      const response = await apiClient.post(\n        `/api/v4/core/${dimension}/batch`,\n        {\n          records: records,\n          transaction: 'atomic'  // All-or-nothing\n        }\n      );\n\n      console.log(`\u2705 All ${records.length} records inserted`);\n      return response;\n    } catch (error) {\n      console.error(`\u274c Batch failed - nothing inserted: ${error.message}`);\n      throw error;\n    }\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"partial-commit-batches",children:"Partial Commit Batches"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"class PartialCommitBatch {\n  async processBatch(dimension, records) {\n    // Process each record independently\n    const response = await apiClient.post(\n      `/api/v4/core/${dimension}/batch`,\n      {\n        records: records,\n        transaction: 'independent'  // Commit each record\n      }\n    );\n\n    console.log(`\u2705 Inserted: ${response.inserted}`);\n    console.log(`\u274c Failed: ${response.failed}`);\n\n    // Process errors\n    for (const error of response.errors) {\n      console.error(`Record ${error.index}: ${error.message}`);\n    }\n\n    return response;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"progress-tracking",children:"Progress Tracking"}),"\n",(0,s.jsx)(n.h3,{id:"progress-bar-implementation",children:"Progress Bar Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"class ProgressTracker {\n  constructor(total) {\n    this.total = total;\n    this.current = 0;\n    this.startTime = Date.now();\n  }\n\n  update(processed) {\n    this.current += processed;\n\n    const percentage = ((this.current / this.total) * 100).toFixed(2);\n    const elapsed = (Date.now() - this.startTime) / 1000;\n    const rate = this.current / elapsed;\n    const remaining = (this.total - this.current) / rate;\n\n    console.log(\n      `Progress: ${this.current}/${this.total} (${percentage}%) | ` +\n      `Rate: ${rate.toFixed(0)} rec/sec | ` +\n      `ETA: ${remaining.toFixed(0)}s`\n    );\n  }\n\n  complete() {\n    const duration = (Date.now() - this.startTime) / 1000;\n    const rate = this.total / duration;\n\n    console.log(\n      `\u2705 Complete: ${this.total} records in ${duration.toFixed(2)}s ` +\n      `(${rate.toFixed(0)} rec/sec)`\n    );\n  }\n}\n\n// Usage\nasync function batchInsertWithProgress(dimension, records) {\n  const batchSize = 100;\n  const batches = chunk(records, batchSize);\n  const tracker = new ProgressTracker(records.length);\n\n  for (const batch of batches) {\n    await apiClient.post(`/api/v4/core/${dimension}/batch`, { records: batch });\n    tracker.update(batch.length);\n  }\n\n  tracker.complete();\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,s.jsx)(n.h3,{id:"retry-failed-batches",children:"Retry Failed Batches"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"class RetryableBatch {\n  constructor(maxRetries = 3) {\n    this.maxRetries = maxRetries;\n  }\n\n  async processBatchWithRetry(dimension, records, attempt = 1) {\n    try {\n      return await apiClient.post(\n        `/api/v4/core/${dimension}/batch`,\n        { records: records }\n      );\n    } catch (error) {\n      if (attempt < this.maxRetries) {\n        const delay = Math.pow(2, attempt) * 1000; // Exponential backoff\n        console.log(`Retry attempt ${attempt} after ${delay}ms...`);\n\n        await this.sleep(delay);\n        return this.processBatchWithRetry(dimension, records, attempt + 1);\n      }\n\n      throw error;\n    }\n  }\n\n  sleep(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"partial-failure-recovery",children:"Partial Failure Recovery"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"class PartialFailureHandler {\n  async processBatch(dimension, records) {\n    const response = await apiClient.post(\n      `/api/v4/core/${dimension}/batch`,\n      {\n        records: records,\n        transaction: 'independent'\n      }\n    );\n\n    if (response.errors.length > 0) {\n      console.log(`${response.errors.length} records failed`);\n\n      // Extract failed records\n      const failedRecords = response.errors.map(err =>\n        records[err.index]\n      );\n\n      // Save to retry queue\n      await this.saveToRetryQueue(dimension, failedRecords);\n\n      // Alert\n      await this.sendAlert({\n        title: 'Batch Processing Partial Failure',\n        dimension: dimension,\n        failed: response.errors.length,\n        total: records.length\n      });\n    }\n\n    return response;\n  }\n\n  async saveToRetryQueue(dimension, records) {\n    await redis.lpush(\n      `retry:${dimension}`,\n      JSON.stringify({\n        records: records,\n        timestamp: Date.now()\n      })\n    );\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"parallel-processing",children:"Parallel Processing"}),"\n",(0,s.jsx)(n.h3,{id:"concurrent-batches",children:"Concurrent Batches"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"class ParallelBatchProcessor {\n  constructor(concurrency = 5) {\n    this.concurrency = concurrency;\n  }\n\n  async processBatches(dimension, records, batchSize = 100) {\n    const batches = chunk(records, batchSize);\n    const results = {\n      inserted: 0,\n      failed: 0,\n      errors: []\n    };\n\n    // Process batches in parallel (max concurrency)\n    for (let i = 0; i < batches.length; i += this.concurrency) {\n      const chunk = batches.slice(i, i + this.concurrency);\n\n      const promises = chunk.map(batch =>\n        apiClient.post(`/api/v4/core/${dimension}/batch`, { records: batch })\n      );\n\n      const responses = await Promise.allSettled(promises);\n\n      // Aggregate results\n      responses.forEach(response => {\n        if (response.status === 'fulfilled') {\n          results.inserted += response.value.inserted;\n          results.failed += response.value.failed;\n        } else {\n          results.failed += batchSize;\n          results.errors.push(response.reason);\n        }\n      });\n    }\n\n    return results;\n  }\n}\n\n// Usage\nconst processor = new ParallelBatchProcessor(5);  // 5 concurrent batches\nawait processor.processBatches('products', records, 100);\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"database-level-optimizations",children:"Database-Level Optimizations"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Disable indexes temporarily:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Before batch insert\nALTER TABLE TB_ANAG_PRD00 DISABLE KEYS;\n\n-- Bulk insert\nINSERT INTO TB_ANAG_PRD00 (...) VALUES (...), (...), ...;\n\n-- Rebuild indexes\nALTER TABLE TB_ANAG_PRD00 ENABLE KEYS;\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Use LOAD DATA INFILE (MySQL):"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Fastest bulk insert method\nLOAD DATA LOCAL INFILE '/tmp/products.csv'\nINTO TABLE TB_ANAG_PRD00\nFIELDS TERMINATED BY ','\nENCLOSED BY '\"'\nLINES TERMINATED BY '\\n'\nIGNORE 1 ROWS\n(XPRD01, XPRD02, XPRD03, XPRD05, XPRD06);\n"})}),"\n",(0,s.jsx)(n.h3,{id:"batch-size-tuning",children:"Batch Size Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// Too small = too many HTTP requests\nconst batchSize = 10;  // \u274c Slow - 1000 batches for 10k records\n\n// Too large = memory issues, long transactions\nconst batchSize = 10000;  // \u274c Risky - single huge transaction\n\n// Optimal = balance throughput and safety\nconst batchSize = 100;  // \u2705 Good - 100 batches for 10k records\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"-do",children:"\u2705 DO:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Use appropriate batch sizes:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// \u2705 Good - 100-500 records per batch\nconst batchSize = 100;\nconst batches = chunk(records, batchSize);\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implement progress tracking:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// \u2705 Good - show progress\nconst tracker = new ProgressTracker(records.length);\nfor (const batch of batches) {\n  await process(batch);\n  tracker.update(batch.length);\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Handle partial failures:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// \u2705 Good - retry failed records\nif (response.errors.length > 0) {\n  await saveToRetryQueue(failedRecords);\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Use transactions appropriately:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// \u2705 Good - atomic for critical operations\n{transaction: 'atomic'}  // All-or-nothing\n\n// \u2705 Good - independent for fault tolerance\n{transaction: 'independent'}  // Continue on errors\n"})}),"\n",(0,s.jsx)(n.h3,{id:"-dont",children:"\u274c DON'T:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Don't process all records in single transaction:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// \u274c Bad - single transaction for 10k records\nawait apiClient.post('/api/v4/core/products/batch', {\n  records: allRecords,  // 10,000 records\n  transaction: 'atomic'\n});\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Don't ignore errors:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// \u274c Bad - no error handling\nfor (const batch of batches) {\n  await process(batch);  // Errors ignored\n}\n\n// \u2705 Good - handle errors\nfor (const batch of batches) {\n  try {\n    await process(batch);\n  } catch (error) {\n    logError(error);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Don't block on sequential processing:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// \u274c Bad - sequential processing\nfor (const batch of batches) {\n  await process(batch);  // Waits for each batch\n}\n\n// \u2705 Good - parallel processing\nawait Promise.all(batches.map(batch => process(batch)));\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Batch processing enables efficient large-scale operations"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Use appropriate batch sizes (100-500 records)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Implement progress tracking and monitoring"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Handle partial failures with retry logic"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Use atomic transactions for critical operations"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Use independent transactions for fault tolerance"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Process batches in parallel for better throughput"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Monitor and alert on batch failures"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Takeaways:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Batch operations dramatically improve throughput"}),"\n",(0,s.jsx)(n.li,{children:"Choose batch size based on record size and complexity"}),"\n",(0,s.jsx)(n.li,{children:"Atomic transactions ensure data consistency"}),"\n",(0,s.jsx)(n.li,{children:"Independent transactions provide fault tolerance"}),"\n",(0,s.jsx)(n.li,{children:"Progress tracking provides visibility"}),"\n",(0,s.jsx)(n.li,{children:"Parallel processing increases throughput"}),"\n",(0,s.jsx)(n.li,{children:"Retry failed batches with exponential backoff"}),"\n",(0,s.jsx)(n.li,{children:"Monitor batch processing metrics"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"related-concepts",children:"Related Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/products/map.q01.io/core-api-platform-v4/write-patterns/create-operations",children:"Write Operations"})," - POST/PUT/PATCH"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/products/map.q01.io/core-api-platform-v4/advanced-topics/performance-optimization",children:"Performance Optimization"})," - Query tuning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/products/map.q01.io/core-api-platform-v4/write-patterns/transactions",children:"Transaction Patterns"})," - ACID compliance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/products/map.q01.io/core-api-platform-v4/advanced-topics",children:"Advanced Topics"})," - Overview"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.M)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},2172:(e,n,r)=>{r.d(n,{I:()=>c,M:()=>i});var s=r(1504);const t={},a=s.createContext(t);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);